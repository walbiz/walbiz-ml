# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cX3MHOEKIXAFFrffLSIjpjAFCEDDCmSr

# EDA and Data Preperocessing

This notebook provides an exploratory data analysis on the franchise webscraping dataset. The collection process is available here in [this notebook](https://drive.google.com/uc?export=download&id=1kCsjLazcZRi8Buy87EX7cjpzySgqPyi7).

# Installing Necessary Packages
"""

!pip install sentence-transformers

"""# Import Libraries"""

import tensorflow as tf
import numpy as np
import pandas as pd
import ast
import matplotlib.pyplot as plt
import seaborn as sns

from pathlib import Path
from sentence_transformers import SentenceTransformer

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter

nltk.download('punkt')
nltk.download('stopwords')

"""# Performing Basic EDA

In this section, we first load the dataset into a pandas.Dataframe and then perform some basic exploratory data analysis (EDA).
"""

franchise_data = pd.read_csv('https://drive.google.com/uc?export=download&id=1rx0KGQh7UvWvx2VmkkMDTRnlTSaMqtss', low_memory=False)

stop_words = set(stopwords.words('indonesian'))
franchise_data['processed_text'] = franchise_data['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

franchise_data.info()
franchise_data.head()

print(f"There are {len(franchise_data)} rows in the dataset.")

"""# Word Cloud of Franchise Names"""

from wordcloud import WordCloud

# Concatenate all titles
all_names = ' '.join(franchise_data['franchise_name'].tolist())

# Create word cloud
wordcloud = WordCloud(background_color = 'white', width=800, height=400).generate(all_names)

# Plot
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""# Word Cloud of Franchise Categories"""

from wordcloud import WordCloud

# Concatenate all titles
all_categories = ' '.join(franchise_data['franchise_category'].tolist())

# Create word cloud
wordcloud = WordCloud(background_color = 'white', width=800, height=400).generate(all_categories)

# Plot
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""# Word Cloud of Franchise Types"""

from wordcloud import WordCloud

# Concatenate all titles
all_types = ' '.join(franchise_data['franchise_type'].tolist())

# Create word cloud
wordcloud = WordCloud(background_color = 'white', width=800, height=400).generate(all_types)

# Plot
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""# Approaches

We will be testing two approaches to recommend papers to a user.

- The first approach is a content-based approach, where we will be recommending franchises based on the similarity of their names.
- The second approach is also a content-based approach but we will be recommending franchises based on the similarity of their description.

# Content-based approach using franchise names

Let's us start by exploring the first approach.

Since we are using the names of the franchises to recommend similar franchises, we can drop the other columns.
"""

names_dataset = franchise_data.drop(columns = ["description"])

names_dataset.head()

average_name_length = int(names_dataset['franchise_name'].apply(len).mean())
print(f"The average text length of a title is {average_name_length} characters.")

# Calculate the length of each title
names_dataset['names_length'] = names_dataset['franchise_name'].apply(len)

# Calculate the min and max length
min_length = names_dataset['names_length'].min()
max_length = names_dataset['names_length'].max()

print('The minimum length of a franchise name:', min_length)
print('The maximum length of a franchise name:', max_length)

def plot_length_distribution(df, column_name) -> None:
    """
    Plots a histogram representing the distribution of lengths in a specified column of a DataFrame.
    The histogram also displays the mean length and one standard deviation above and below the mean.

    Args:
        df (pd.DataFrame): The DataFrame containing the data.
        column_name (str): The name of the column in the DataFrame for which to plot the length distribution.

    Returns:
        None. This function outputs a plot.
    """

    # Compute the lengths of all titles
    names_lengths = df[column_name].apply(len)

    # Calculate mean and standard deviation
    mean_length = names_lengths.mean()
    std_length = names_lengths.std()

    # Plot the histogram
    plt.figure(figsize=(10, 6))
    sns.histplot(names_lengths, bins=50, color='b', alpha=0.2)

    # Add lines for the mean and standard deviation
    plt.axvline(mean_length, color='r', linestyle='-', linewidth=1.5)
    plt.axvline(mean_length - std_length, color='gray',
                linestyle='--', linewidth=1)
    plt.axvline(mean_length + std_length, color='gray',
                linestyle='--', linewidth=1)

    # Add a text box with the mean value
    plt.text(mean_length+5, plt.gca().get_ylim()
             [1]*0.9, f"Mean: {mean_length:.2f}", fontsize=10)

    plt.title('Distribution of ' + column_name.capitalize() + ' Lengths')
    plt.xlabel(column_name.capitalize() + ' Length')
    plt.ylabel('Frequency')
    plt.legend(['Mean', 'Standard Deviation'])
    plt.show()

def plot_top_words(df, column_name) -> None:
    """
    Plot the top 10 most common words in a specified column of a DataFrame.

    The function tokenizes the strings, converts to lower case, removes non-alphabetic tokens
    and stop words, counts the frequency of each word, and then plots the 10 most common words
    using a horizontal bar plot.

    Args:
        df (pd.DataFrame): The DataFrame containing the text data.
        column_name (str): The column of the DataFrame to analyze.

    Returns:
        None. The function shows a plot.
    """

    # Create a single string containing all sentences
    all_setences= " ".join(df[column_name].values)

    # Tokenize the string
    tokens = word_tokenize(all_setences)

    # Convert to lower case
    tokens = [word.lower() for word in tokens]

    # Remove non-alphabetic tokens and stop words
    words = [word for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if not word in stop_words]

    # Count the frequency of each word
    counter = Counter(words)

    # Get the 10 most common words
    most_common = counter.most_common(10)

    # Create a DataFrame from the most common words
    most_common_df = pd.DataFrame(most_common, columns=['Word', 'Frequency'])

    # Plot the results using seaborn
    plt.figure(figsize=(10,6))
    sns.barplot(y='Word', x='Frequency', data=most_common_df, palette='viridis', orient='h')

    # Change font size
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)

    plt.title(f'Top 10 Words in {column_name} column', fontsize=16)
    plt.show()

plot_length_distribution(names_dataset, 'franchise_name')

plot_top_words(names_dataset, 'franchise_name')

"""# Content-based approach using descriptions

Let's us now explore the second approach.

We will be dropping the franchise_name column and keeping the description column.
"""

description_dataset = franchise_data.drop(columns = ["franchise_name"])
description_dataset.head()

average_description_length = int(description_dataset['description'].apply(len).mean())
print(f"The average text length of an description is {average_description_length} characters.")

# Calculate the length of each description
description_dataset['description_length'] = description_dataset['description'].apply(len)

# Calculate the min and max length
min_length = description_dataset['description_length'].min()
max_length = description_dataset['description_length'].max()

print('The minimum length of an description:', min_length)
print('The maximum length of an description:', max_length)

def plot_length_distribution(df, column_name) -> None:
    """
    Plots a histogram representing the distribution of lengths in a specified column of a DataFrame.
    The histogram also displays the mean length and one standard deviation above and below the mean.

    Args:
        df (pd.DataFrame): The DataFrame containing the data.
        column_name (str): The name of the column in the DataFrame for which to plot the length distribution.

    Returns:
        None. This function outputs a plot.
    """

    # Compute the lengths of all titles
    description_lengths = df[column_name].apply(len)

    # Calculate mean and standard deviation
    mean_length = description_lengths.mean()
    std_length = description_lengths.std()

    # Plot the histogram
    plt.figure(figsize=(10, 6))
    sns.histplot(description_lengths, bins=50, color='b', alpha=0.2)

    # Add lines for the mean and standard deviation
    plt.axvline(mean_length, color='r', linestyle='-', linewidth=1.5)
    plt.axvline(mean_length - std_length, color='gray',
                linestyle='--', linewidth=1)
    plt.axvline(mean_length + std_length, color='gray',
                linestyle='--', linewidth=1)

    # Add a text box with the mean value
    plt.text(mean_length+5, plt.gca().get_ylim()
             [1]*0.9, f"Mean: {mean_length:.2f}", fontsize=10)

    plt.title('Distribution of ' + column_name.capitalize() + ' Lengths')
    plt.xlabel(column_name.capitalize() + ' Length')
    plt.ylabel('Frequency')
    plt.legend(['Mean', 'Standard Deviation'])
    plt.show()

def plot_top_words(df, column_name) -> None:
    """
    Plot the top 10 most common words in a specified column of a DataFrame.

    The function tokenizes the strings, converts to lower case, removes non-alphabetic tokens
    and stop words, counts the frequency of each word, and then plots the 10 most common words
    using a horizontal bar plot.

    Args:
        df (pd.DataFrame): The DataFrame containing the text data.
        column_name (str): The column of the DataFrame to analyze.

    Returns:
        None. The function shows a plot.
    """

    # Create a single string containing all sentences
    all_setences= " ".join(df[column_name].values)

    # Tokenize the string
    tokens = word_tokenize(all_setences)

    # Convert to lower case
    tokens = [word.lower() for word in tokens]

    # Remove non-alphabetic tokens and stop words
    words = [word for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if not word in stop_words]

    # Count the frequency of each word
    counter = Counter(words)

    # Get the 10 most common words
    most_common = counter.most_common(10)

    # Create a DataFrame from the most common words
    most_common_df = pd.DataFrame(most_common, columns=['Word', 'Frequency'])

    # Plot the results using seaborn
    plt.figure(figsize=(10,6))
    sns.barplot(y='Word', x='Frequency', data=most_common_df, palette='viridis', orient='h')

    # Change font size
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)

    plt.title(f'Top 10 Words in {column_name} column', fontsize=16)
    plt.show()

plot_length_distribution(description_dataset, 'description')

plot_top_words(description_dataset, 'description')

"""# Content-based approach using categories"""

category_dataset = franchise_data.drop(columns = ["franchise_name"])
category_dataset.head()

average_category_length = int(category_dataset['franchise_category'].apply(len).mean())
print(f"The average text length of an category is {average_category_length} characters.")

# Fill None values with empty strings in 'franchise_category' column
category_dataset['franchise_category'].fillna('', inplace=True)

# Calculate the length of each category
category_dataset['category_length'] = category_dataset['franchise_category'].apply(len)

# Calculate the min and max length
min_length = category_dataset['category_length'].min()
max_length = category_dataset['category_length'].max()

print('The minimum length of a category:', min_length)
print('The maximum length of a category:', max_length)

def plot_length_distribution(df, column_name) -> None:
    """
    Plots a histogram representing the distribution of lengths in a specified column of a DataFrame.
    The histogram also displays the mean length and one standard deviation above and below the mean.

    Args:
        df (pd.DataFrame): The DataFrame containing the data.
        column_name (str): The name of the column in the DataFrame for which to plot the length distribution.

    Returns:
        None. This function outputs a plot.
    """

    # Compute the lengths of all titles
    category_lengths = df[column_name].apply(len)

    # Calculate mean and standard deviation
    mean_length = category_lengths.mean()
    std_length = category_lengths.std()

    # Plot the histogram
    plt.figure(figsize=(10, 6))
    sns.histplot(category_lengths, bins=50, color='b', alpha=0.2)

    # Add lines for the mean and standard deviation
    plt.axvline(mean_length, color='r', linestyle='-', linewidth=1.5)
    plt.axvline(mean_length - std_length, color='gray',
                linestyle='--', linewidth=1)
    plt.axvline(mean_length + std_length, color='gray',
                linestyle='--', linewidth=1)

    # Add a text box with the mean value
    plt.text(mean_length+5, plt.gca().get_ylim()
             [1]*0.9, f"Mean: {mean_length:.2f}", fontsize=10)

    plt.title('Distribution of ' + column_name.capitalize() + ' Lengths')
    plt.xlabel(column_name.capitalize() + ' Length')
    plt.ylabel('Frequency')
    plt.legend(['Mean', 'Standard Deviation'])
    plt.show()

def plot_top_words(df, column_name) -> None:
    """
    Plot the top 10 most common words in a specified column of a DataFrame.

    The function tokenizes the strings, converts to lower case, removes non-alphabetic tokens
    and stop words, counts the frequency of each word, and then plots the 10 most common words
    using a horizontal bar plot.

    Args:
        df (pd.DataFrame): The DataFrame containing the text data.
        column_name (str): The column of the DataFrame to analyze.

    Returns:
        None. The function shows a plot.
    """

    # Create a single string containing all sentences
    all_setences= " ".join(df[column_name].values)

    # Tokenize the string
    tokens = word_tokenize(all_setences)

    # Convert to lower case
    tokens = [word.lower() for word in tokens]

    # Remove non-alphabetic tokens and stop words
    words = [word for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if not word in stop_words]

    # Count the frequency of each word
    counter = Counter(words)

    # Get the 10 most common words
    most_common = counter.most_common(10)

    # Create a DataFrame from the most common words
    most_common_df = pd.DataFrame(most_common, columns=['Word', 'Frequency'])

    # Plot the results using seaborn
    plt.figure(figsize=(10,6))
    sns.barplot(y='Word', x='Frequency', data=most_common_df, palette='viridis', orient='h')

    # Change font size
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)

    plt.title(f'Top 10 Words in {column_name} column', fontsize=16)
    plt.show()

plot_length_distribution(category_dataset, 'franchise_category')

plot_top_words(category_dataset, 'franchise_category')

"""# Content-based approach using types"""

type_dataset = franchise_data.drop(columns = ["franchise_name"])
type_dataset.head()

average_type_length = int(type_dataset['franchise_type'].apply(len).mean())
print(f"The average text length of a type is {average_type_length} characters.")

# Calculate the length of each type
type_dataset['type_length'] = type_dataset['franchise_type'].apply(len)

# Calculate the min and max length
min_length = type_dataset['type_length'].min()
max_length = type_dataset['type_length'].max()

print('The minimum length of a type:', min_length)
print('The maximum length of a type:', max_length)

def plot_length_distribution(df, column_name) -> None:
    """
    Plots a histogram representing the distribution of lengths in a specified column of a DataFrame.
    The histogram also displays the mean length and one standard deviation above and below the mean.

    Args:
        df (pd.DataFrame): The DataFrame containing the data.
        column_name (str): The name of the column in the DataFrame for which to plot the length distribution.

    Returns:
        None. This function outputs a plot.
    """

    # Compute the lengths of all titles
    type_lengths = df[column_name].apply(len)

    # Calculate mean and standard deviation
    mean_length = type_lengths.mean()
    std_length = type_lengths.std()

    # Plot the histogram
    plt.figure(figsize=(10, 6))
    sns.histplot(type_lengths, bins=50, color='b', alpha=0.2)

    # Add lines for the mean and standard deviation
    plt.axvline(mean_length, color='r', linestyle='-', linewidth=1.5)
    plt.axvline(mean_length - std_length, color='gray',
                linestyle='--', linewidth=1)
    plt.axvline(mean_length + std_length, color='gray',
                linestyle='--', linewidth=1)

    # Add a text box with the mean value
    plt.text(mean_length+5, plt.gca().get_ylim()
             [1]*0.9, f"Mean: {mean_length:.2f}", fontsize=10)

    plt.title('Distribution of ' + column_name.capitalize() + ' Lengths')
    plt.xlabel(column_name.capitalize() + ' Length')
    plt.ylabel('Frequency')
    plt.legend(['Mean', 'Standard Deviation'])
    plt.show()

def plot_top_words(df, column_name) -> None:
    """
    Plot the top 10 most common words in a specified column of a DataFrame.

    The function tokenizes the strings, converts to lower case, removes non-alphabetic tokens
    and stop words, counts the frequency of each word, and then plots the 10 most common words
    using a horizontal bar plot.

    Args:
        df (pd.DataFrame): The DataFrame containing the text data.
        column_name (str): The column of the DataFrame to analyze.

    Returns:
        None. The function shows a plot.
    """

    # Create a single string containing all sentences
    all_setences= " ".join(df[column_name].values)

    # Tokenize the string
    tokens = word_tokenize(all_setences)

    # Convert to lower case
    tokens = [word.lower() for word in tokens]

    # Remove non-alphabetic tokens and stop words
    words = [word for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if not word in stop_words]

    # Count the frequency of each word
    counter = Counter(words)

    # Get the 10 most common words
    most_common = counter.most_common(10)

    # Create a DataFrame from the most common words
    most_common_df = pd.DataFrame(most_common, columns=['Word', 'Frequency'])

    # Plot the results using seaborn
    plt.figure(figsize=(10,6))
    sns.barplot(y='Word', x='Frequency', data=most_common_df, palette='viridis', orient='h')

    # Change font size
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)

    plt.title(f'Top 10 Words in {column_name} column', fontsize=16)
    plt.show()

plot_length_distribution(type_dataset, 'franchise_type')

plot_top_words(type_dataset, 'franchise_type')

"""# Content-based approach using costs"""

cost_dataset = franchise_data.drop(columns = ["franchise_name"])
cost_dataset.head()

# Convert 'costs' column to strings and handle NaN values
cost_dataset['costs'] = cost_dataset['costs'].astype(str)

# Calculate the average length of the 'costs' column
average_cost_length = int(cost_dataset['costs'].apply(len).mean())
print(f"The average text length of a cost is {average_cost_length} characters.")

# Calculate the length of each title
cost_dataset['cost_length'] = cost_dataset['costs'].apply(len)

# Calculate the min and max length
min_length = cost_dataset['cost_length'].min()
max_length = cost_dataset['cost_length'].max()

print('The minimum length of a cost:', min_length)
print('The maximum length of a cost:', max_length)

def plot_length_distribution(df, column_name) -> None:
    """
    Plots a histogram representing the distribution of lengths in a specified column of a DataFrame.
    The histogram also displays the mean length and one standard deviation above and below the mean.

    Args:
        df (pd.DataFrame): The DataFrame containing the data.
        column_name (str): The name of the column in the DataFrame for which to plot the length distribution.

    Returns:
        None. This function outputs a plot.
    """

    # Compute the lengths of all titles
    cost_lengths = df[column_name].apply(len)

    # Calculate mean and standard deviation
    mean_length = cost_lengths.mean()
    std_length = cost_lengths.std()

    # Plot the histogram
    plt.figure(figsize=(10, 6))
    sns.histplot(cost_lengths, bins=50, color='b', alpha=0.2)

    # Add lines for the mean and standard deviation
    plt.axvline(mean_length, color='r', linestyle='-', linewidth=1.5)
    plt.axvline(mean_length - std_length, color='gray',
                linestyle='--', linewidth=1)
    plt.axvline(mean_length + std_length, color='gray',
                linestyle='--', linewidth=1)

    # Add a text box with the mean value
    plt.text(mean_length+5, plt.gca().get_ylim()
             [1]*0.9, f"Mean: {mean_length:.2f}", fontsize=10)

    plt.title('Distribution of ' + column_name.capitalize() + ' Lengths')
    plt.xlabel(column_name.capitalize() + ' Length')
    plt.ylabel('Frequency')
    plt.legend(['Mean', 'Standard Deviation'])
    plt.show()

def plot_top_words(df, column_name) -> None:
    """
    Plot the top 10 most common words in a specified column of a DataFrame.

    The function tokenizes the strings, converts to lower case, removes non-alphabetic tokens
    and stop words, counts the frequency of each word, and then plots the 10 most common words
    using a horizontal bar plot.

    Args:
        df (pd.DataFrame): The DataFrame containing the text data.
        column_name (str): The column of the DataFrame to analyze.

    Returns:
        None. The function shows a plot.
    """

    # Create a single string containing all sentences
    all_setences= " ".join(df[column_name].values)

    # Tokenize the string
    tokens = word_tokenize(all_setences)

    # Convert to lower case
    tokens = [word.lower() for word in tokens]

    # Remove non-alphabetic tokens and stop words
    words = [word for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if not word in stop_words]

    # Count the frequency of each word
    counter = Counter(words)

    # Get the 10 most common words
    most_common = counter.most_common(10)

    # Create a DataFrame from the most common words
    most_common_df = pd.DataFrame(most_common, columns=['Word', 'Frequency'])

    # Plot the results using seaborn
    plt.figure(figsize=(10,6))
    sns.barplot(y='Word', x='Frequency', data=most_common_df, palette='viridis', orient='h')

    # Change font size
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)

    plt.title(f'Top 10 Words in {column_name} column', fontsize=16)
    plt.show()

plot_length_distribution(cost_dataset, 'costs')

"""# Sentence Embeddings

This notebook contains the code to generate sentence embeddings using the pre-trained models from the [sentence-transformers](https://www.sbert.net/index.html) library.
"""

#model = SentenceTransformer('all-MiniLM-L6-v2')

# Our feature we like to encode
#sentences = franchise_data['franchise_name']

# Features are encoded by calling model.encode()
#embeddings = model.encode(sentences)

model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Our feature we like to encode
sentences = franchise_data['franchise_name']

# Features are encoded by calling model.encode()
embeddings = model.encode(franchise_data['processed_text'].tolist())

# Print the embeddings
c = 0
for sentence, embedding in zip(sentences, embeddings):

    print("Sentence:", sentence)
    print("Embedding dimension:", len(embedding))
    print("Franchise Name length:", len(sentence))
    print("")

    if c >=5:
        break
    c +=1

import pickle

#menyimpan embeddings ke file
with open('embedding.pkl', 'wb') as file:
  pickle.dump(embeddings, file)

with open('sentences.pkl', 'wb') as file:
  pickle.dump(sentences, file)

"""# Testing the embedding model"""

franchise_you_like = input("Enter your business of interest here ðŸ‘‡ \n")
franchise_you_like

from sentence_transformers import util
cosine_scores = util.cos_sim(embeddings, model.encode(franchise_you_like))

import torch
top_similar_franchise = torch.topk(cosine_scores,dim=0, k=5,sorted=True)
top_similar_franchise

for i in top_similar_franchise.indices:
#     print(i)
    print(sentences[i.item()])

